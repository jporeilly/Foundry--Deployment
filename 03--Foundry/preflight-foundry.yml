---
# Preflight - Foundry platform
- name: Preflight Foundry Cluster
  hosts:  "{{ groups['kube-node'] }}"
  become: true
  become_method: sudo
  become_user: root
  gather_facts: true
  vars:
    ansible_ssh_private_key_file: "~/.ssh/id_rsa"
    ansible_ssh_private_key_file_name: "id_rsa"
    ansible_user: k8s

    ansible_python_interpreter: /usr/bin/python3

  tasks:
    # Ping Nodes
    - name: Ping Nodes
      ping:
      tags: 
       - info
   
    # Update Packages
    - name: Update all installed packages using YUM module
      yum:
       name: '*'
       state: latest
       update_cache: yes
       update_only: yes
      register: yum_update_status
      tags: 
      - updates
    
    # Retrieve sudoers path
    - name: Get sudoers secure_path
      shell: cat /etc/sudoers | grep secure_path
      register: sudoers_path
        
    # Require /usr/local/bin in the sudoers path so 'su' will find kubectl and Helm3.
    - name: Update sudoers secure_path to include /usr/local/bin
      shell: |
        sed -i "s!{{ sudoers_path.stdout }}!{{ sudoers_path.stdout }}:/usr/local/bin!g" /etc/sudoers     
      when: "'/usr/local/bin' not in sudoers_path.stdout"

    # Reboot Nodes and Test
    - name: Reboot hosts if required
      reboot:
        msg: "Reboot initiated by Ansible"
        connect_timeout: 5
        reboot_timeout: 600
        pre_reboot_delay: 0
        post_reboot_delay: 30
        test_command: whoami
      when: yum_update_status.changed

# Install Helm3 on all hosts 
# Not strictly required as Helm is already installed in the 'install-pod', however used to issue
# Helm commands from other servers in the environment.
# Should be a preflight task.
- name: Install helm on all hosts 
  hosts: all
  become: true
  become_method: sudo
  become_user: root
  gather_facts: false
  vars:
    ansible_python_interpreter: /usr/bin/python3

  tasks:
    # Check for Helm. 
    # The SSH connection to the haproxy server will fail as no keys have been copied over.
    - name: Check Helm version
      shell: helm version
      register: helm_version
      ignore_errors: true
      tags: 
       - helm

    - name: Get Helm
      get_url:
         url: https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
         dest: ./get_helm.sh
         mode: 700
      when: helm_version.rc != 0
      tags: 
       - helm
       
    - name: Install Helm
      shell: ./get_helm.sh
      when: "'not found' in helm_version.stderr"
      tags: 
       - helm

# Switch to Master Node 1 as root
# Configure kubeconfig on cluster for {{ ansible_user }}
- name: Prepare .kube/config for {{ ansible_user }}
  hosts:  "{{ groups['kube-master'][0] }}"
  become: true
  become_method: sudo
  gather_facts: false
  any_errors_fatal: true
  vars:
    ansible_ssh_private_key_file: "~/.ssh/id_rsa"
    ansible_ssh_private_key_file_name: "id_rsa"
    ansible_user: k8s

    ansible_python_interpreter: /usr/bin/python3
  tags: kubeconfig
  
  tasks:
    - name: Create ~/.kube directory (as 'root' on master-node-01)
      file:
        path: ~/.kube
        state: directory
      become: false

    # Copy config to {{ansible_user}} ~/.kube directory
    - name: Copy config to {{ ansible_user }} home
      shell: "cp ~/.kube/config /home/{{ ansible_user }}/.kube/"

    # Take ownership
    - name: Change config ownership to {{ ansible_user }}
      shell: "chown -R {{ ansible_user }}:{{ ansible_user }} /home/{{ ansible_user }}/.kube/config"

# Switch to Ansible Controller
# Setup kubectl for local user and root by copying from the cluster
- name: Setup kubectl for '{{ ansible_user }}' user on installer node
  hosts: "{{ groups['installer'][0] }}"
  become: true
  become_user: root
  become_method: sudo
  gather_facts: true
  any_errors_fatal: true
  vars:
    ansible_ssh_private_key_file: "~/.ssh/id_rsa"
    ansible_ssh_private_key_file_name: "id_rsa"
    ansible_user: k8s

    ansible_python_interpreter: /usr/bin/python3
  tags: kubeconfig
  
  tasks:
    - name: Install jq 
      apt:
        name: jq
      when: ansible_distribution == "Ubuntu"

    - name: Install kubectl
      raw: snap install kubectl --channel=1.21/stable --classic

    - name: Create ~/.kube directory (as 'root' on Ansible Controller)
      file:
        path: ~/.kube
        state: directory
      become: false

    - name: Transfer config file from master cluster to 'root' .kube on Ansible Controller
      raw: scp -i {{ ansible_ssh_private_key_file }} {{ ansible_user }}@{{ groups['kube-master'][0] }}:/home/{{ ansible_user }}/.kube/config ~/.kube/config
      become: false

    # Update IP address in ~/.kube/config to master-node-01.skytap.example
    - lineinfile: 
        path: /home/installer/.kube/config
        regexp: '^(.*)server:(.*)$'
        line: 'server: https://master-node-01.skytap.example:6443'
        backup: true    

    # Create ~/.kube directory
    - name: Create ~/.kube directory
      file:
        path: ~/.kube
        state: directory

    # Copy over root/.kube/config to home/installer/.kube
    - name: Copy config from root to home
      copy:
       src: ~/.kube/config
       dest: /home/installer/.kube/config
       owner: installer
       group: installer
       mode: '0644'

    # Test kubectl config
    - name: Check kubectl connectivity from installer node to cluster
      shell: "kubectl get nodes"
      register: nodes
      become: false

    # Display the Nodes
    - name: Show output from command --> kubectl get nodes
      debug:
        msg: "{{ nodes.stdout_lines }}"

    - name: Check kubectl connectivity from installer node to cluster
      shell: "kubectl get pods -n kube-system"
      register: pods
      become: false

    # Display the Pods
    - name: Show output from command --> kubectl get pods -n kube-system
      debug:
        msg: "{{ pods.stdout_lines }}"